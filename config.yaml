# Configuration file for the ETL pipeline

# Data sources
data_sources:
  - name: example_api
    type: api
    endpoint: "https://api.example.com/data"
    headers:
      Authorization: "Bearer YOUR_API_KEY"

  - name: local_csv
    type: file
    path: "data/input.csv"

# Transformations
transformations:
  - name: clean_data
    type: drop_nulls
    columns: ["column1", "column2"]

  - name: add_calculated_column
    type: calculate
    formula: "column1 * column2"

# Data destinations
data_destinations:
  - name: database
    type: postgres
    connection_string: "postgresql://user:password@localhost:5432/mydatabase"

  - name: local_output
    type: file
    path: "data/output.csv"

# Headers for the API request
headers:
  Authorization: "Bearer YOUR_API_KEY"
  Accept: "application/json"
  Content-Type: "application/json"
  User-Agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
  Accept-Encoding: "gzip, deflate, br"